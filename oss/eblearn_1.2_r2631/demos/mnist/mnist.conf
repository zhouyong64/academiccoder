# paths ########################################################################
name            = mnist
ebl             = ${HOME}/eblearn/          # eblearn root
root            = ${HOME}/mnist/            # mnist data root
tblroot         = ${ebl}/tools/data/tables/ # location of table files

# network high level switches ##################################################
classifier_type = cf    # type of classifier, cf: 2-layer, c: 1-layer
classifier_hidden = 16  # number of hidden units in 2-layer classifier
features_type   = cscs  # type of features
nonlin          = tanh  # type of non-linearity
manual_load     = 0     # manually load weights for individual modules
norm            = 1     # use normalization layers
pool            = subs  # l2pool (is another option)
run_type        = train # detect fprop

# features and classifiers #####################################################
arch            = ${pp},${arch_${run_type}} # run_type is set by runned tool
arch_train      = ${features},${classifier}
arch_detect     = ${arch_train}
arch_fprop      = ${features}
arch_name       = ${arch_name_${run_type}}
arch_name_fprop = ${features_name}
arch_name_train = ${features},${classifier}
features        = ${features_${features_type}}
features_name   = ${features_type}
classifier      = ${classifier_${classifier_type}}

# energies & answers ###########################################################
trainer         = trainable_module1  # the trainer module
trainable_module1_energy = l2_energy # type of energy
answer          = class_answer # how to infer answers from network raw outputs

# normalization ################################################################
norm0_0         =       # no normalization
norm2_0         =       # no normalization
norm0_1         = wstd0 # contrast normalization
norm2_1         = wstd2 # contrast normalization

# architecture #################################################################

# features
features_cscs   = ${c0},${s1},${c2},${s3}

# classifiers
classifier_c    = ${c5}
classifier_cf   = ${c5},${f7}

# main branch layers
c0              = conv0,addc0,${nonlin},abs0,${norm0_${norm}}
s1              = ${pool}1,addc1,${nonlin}
c2              = conv2,addc2,${nonlin},abs2,${norm2_${norm}}
s3              = ${pool}3,addc3,${nonlin}
c5              = conv5,addc5,${nonlin}
f6              = linear6,addc6,${nonlin}
f7              = linear7,addc7,${nonlin}

# main branch parameters
zpad0_dims      = 5x5 # padding for this kernel size
conv0_kernel    = 5x5 # convolution kernel sizes (hxw)
conv0_stride    = 1x1 # convolution strides  (hxw)
conv0_table     =     # convolution table (optional)
conv0_table_in  = 1   # conv input max, used if table file not defined
conv0_table_out = 6   # features max, used if table file not defined
conv0_weights   =     # manual loading of weights (optional)
addc0_weights   =     # manual loading of weights (optional)
wstd0_kernel    = ${conv0_kernel} # normalization kernel sizes (hxw)
subs1_kernel    = 2x2 # subsampling kernel sizes (hxw)
subs1_stride    = ${subs1_kernel} # subsampling strides (hxw)
l2pool1_kernel  = 2x2 # subsampling kernel sizes (hxw)
l2pool1_stride  = ${l2pool1_kernel} # subsampling strides (hxw)
addc1_weights   = # manual loading of weights (optional)
conv2_kernel    = 5x5 # convolution kernel sizes (hxw)
conv2_stride    = 1x1 # convolution strides (hxw)
conv2_table     = ${tblroot}/table_6_16_connect_60.mat # conv table (optional)
conv2_table_in  = thickness # use current thickness as max table input
conv2_table_out = ${table1_max} # features max, used if table file not defined
conv2_weights   =     # manual loading of weights (optional)
addc2_weights   =     # manual loading of weights (optional)
wstd2_kernel    = ${conv2_kernel} # normalization kernel sizes (hxw)
subs3_kernel    = 2x2 # subsampling kernel sizes (hxw)
subs3_stride    = ${subs3_kernel} # subsampling strides (hxw)
l2pool3_kernel    = 2x2 # l2poolampling kernel sizes (hxw)
l2pool3_stride    = ${l2pool3_kernel} # subsampling strides (hxw)
addc3_weights   =     # manual loading of weights (optional)
linear5_in      = ${linear5_in_${net}} # linear module input features size
linear5_out     = noutputs # use number of classes as max table output
linear6_in      = thickness # linear module input features size
linear6_out     = ${classifier_hidden}
linear7_in      = thickness # use current thickness
linear7_out     = noutputs # use number of classes as max table output
conv5_kernel    = 5x5 # convolution kernel sizes (hxw)
conv5_stride    = 1x1 # convolution strides (hxw)
conv5_table_in  = thickness # use current thickness as max table input
conv5_table_out = 120 # features max, used if table file not defined

# preprocessing ################################################################
pp              = ${pp_${run_type}}
pp_train        = zpad0
pp_fprop        = zpad0
pp_detect       = zpad0

# possible preprocessings
pp_y            = rgb_to_y0
pp_yuv          = rgb_to_yuv0
pp_yp           = rgb_to_yp0
pp_ypuv         = rgb_to_ypuv0
rgb_to_ypuv0_kernel = 7x7
resizepp0_pp    = rgb_to_ypuv0
resizepp0_zpad  = 2x2x2x2

# training #####################################################################
classification  = 1 # load datasets in classification mode, regression otherwise
train           = ${root}/train-images-idx3-ubyte # training data
train_labels    = ${root}/train-labels-idx1-ubyte # training labels
train_size      = 2000                            # limit number of samples
val             = ${root}/t10k-images-idx3-ubyte  # validation data
val_labels      = ${root}/t10k-labels-idx1-ubyte  # validation labels
val_size        = 1000                            # limit number of samples
#data_bias       = -128                            # bias applied before coeff
data_coeff      = .01                             # coeff applied after bias
add_features_dimension = 1 # samples are 28x28, make them 1x28x28
test_display_modulo = 0  # modulo at which to display test results

# hyper-parameters
eta             = .0001  # learning rate
reg             = 0      # regularization
reg_l1          = ${reg} # L1 regularization
reg_l2          = ${reg} # L2 regularization
reg_time        = 0      # time (in samples) after which to start regularizing
inertia         = 0.0    # gradient inertia
anneal_value    = 0.0    # learning rate decay value
anneal_period   = 0      # period (in samples) at which to decay learning rate
gradient_threshold = 0.0
iterations      = 20     # number of training iterations
ndiaghessian    = 100    # number of sample for 2nd derivatives estimation
epoch_mode      = 1      # 0: fixed number 1: show all at least once
#epoch_size = 4000    # number of training samples per epoch. comment to ignore.
epoch_show_modulo = 400  # print message every n training samples
sample_probabilities = 0 # use probabilities to pick samples
hardest_focus   = 1      # 0: focus on easiest samples 1: focus on hardest ones
ignore_correct  = 0      # If 1, do not train on correctly classified samples
min_sample_weight = 0    # minimum probability of each sample
per_class_norm  = 1      # normalize probabiliy by class (1) or globally (0)
shuffle_passes  = 1      # shuffle samples between passes
balanced_training = 1    # show each class the same amount of samples or not
random_class_order = 0   # class order is randomized or not when balanced
no_training_test = 0     # do not test on training set if 1
no_testing_test = 0      # do not test on testing set if 1
max_testing     = 0      # limit testing to this number of samples
save_pickings   = 0      # save sample picking statistics
binary_target   = 0      # use only 1 output, -1 is negative, +1 positive
test_only       = 0      # if 1, just test the data and return
save_weights    = 0      # if 0, do not save weights after each iteration
keep_outputs    = 1      # keep all outputs in memory
training_precision = double #float
save_weights    = 1      # if 0, do not save weights when training

# training display #############################################################
display               = 1  # display results
show_conf             = 1  # show configuration variables or not
show_train            = 0  # enable/disable all training display
show_train_ninternals = 1  # number of internal examples to display
show_train_errors     = 0  # show worst errors on training set
show_train_correct    = 0  # show worst corrects on training set
show_val_errors       = 1  # show worst errors on validation set
show_val_correct      = 1  # show worst corrects on validation set
show_hsample          = 5  # number of samples to show on height axis
show_wsample          = 18 # number of samples to show on height axis
show_wait_user        = 0  # if 1, wait for user to close windows

# detection ####################################################################
classes      = ${ebl}/demos/mnist/mnist_classes.mat # names and # of classes
scaling_type = 4           # 4: no multi-scaling, just original 28x28 image
input_gain   = ${data_coeff} # use same gain as in preprocessed training data
camera       = directory   # input images come from a directory
input_dir    = ${ebl}/tools/data/mnist/ # directory for input images
#next_on_key  = n   # with focus on display window, press "n" to process next
weights      = mnist_net00010.mat

################################################################################
